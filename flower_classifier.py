# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bWfiHCzE8bR3mxeLQ5nx7KIX1NFSxg8k
"""

!pip install -q kaggle

from google.colab import files

files.upload()

!mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets list

!kaggle datasets download -d saidakbarp/17-category-flowers

!unzip '/content/17-category-flowers.zip'

!tar -xvf /content/17flowers.tgz

import tarfile
from keras import applications
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
from keras import applications
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.applications.vgg16 import preprocess_input, VGG16
from keras.optimizers import SGD
from keras.models import Sequential, Model
import numpy as np
import pandas as pd
import shutil
import os

#o = tarfile.open("../content/17flowers.tgz")
#o.extractall()

#80 images for each class. Class 0 data is from 0-79, Class 1 is 80-159

j = 0
total = 1361
for i  in range(1, total):
    fpath = f"jpg/image_{str(i).zfill(4)}.jpg"
    destPath = 'flower_dataset/'+str(j).zfill(2)
    if not os.path.exists(destPath):
        os.makedirs(destPath)
    shutil.copy(fpath, destPath)

    if i%80==0:
        j+=1

import matplotlib.pyplot as plt

img = load_img('flower_dataset/09/image_0761.jpg')
x = img_to_array(img)
g = plt.imshow(x/255.)

# VGG 
from keras.applications import VGG16

img_rows = 224
img_cols = 224

vgg16 = VGG16(weights='imagenet',
              include_top = False,
              input_shape = (img_rows,img_cols,3))

for layer in vgg16.layers:
  layer.trainable = False

for (i,layer) in enumerate(vgg16.layers):
  print(str(i) + " " + layer.__class__.__name__,layer.trainable)

def addTopModel(bottom_model,num_classes,D=256):
  """
  creates the top or head of the model for FC
  """
  top_model = bottom_model.output
  top_model = Flatten(name = 'flatten')(top_model)  
  top_model = Dense(D,activation = 'relu')(top_model)
  top_model = Dropout(0.3)(top_model)
  top_model = Dense(num_classes,activation = 'softmax')(top_model)
  return top_model

num_classes = 17

FC_Head = addTopModel(vgg16 , num_classes)

model = Model(inputs = vgg16.input , outputs = FC_Head)

print(model.summary())

batch_size = 16

#Data Augmentation
train_datagen = ImageDataGenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        validation_split=0.2)


#Normalize
test_datagen = ImageDataGenerator(rescale=1./255)


train_generator = train_datagen.flow_from_directory(
        '/content/flower_dataset',  
        target_size=(224, 224),  
        batch_size=batch_size,
        class_mode='categorical',
        subset="training")  

validation_generator = train_datagen.flow_from_directory(
        '/content/flower_dataset',  
        target_size=(224, 224),  
        batch_size=batch_size,
        class_mode='categorical',
        subset="validation") 

nb_train_sample = 1088
nb_validation_sample = 272

from keras.optimizers import RMSprop
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.preprocessing.image import ImageDataGenerator

checkpoint = ModelCheckpoint('/content/Checkpoints/flower_mobileNet.h5',
                             monitor = 'val_loss',
                             mode = 'min',
                             save_best_only = True,
                             verbose = 1)

earlystop = EarlyStopping(monitor = 'val_loss',
                          min_delta = 0,
                          patience = 3,
                          verbose = 1,
                          restore_best_weights = True)

callbacks = [earlystop , checkpoint]

model.compile(loss = 'categorical_crossentropy',
              optimizer = RMSprop(lr = 0.001),
              metrics = ['accuracy'])

epochs = 3
batch_size = 16

history = model.fit_generator(
    train_generator,
    steps_per_epoch = nb_train_sample // batch_size,
    epochs = epochs,
    callbacks = callbacks,
    validation_data = validation_generator,
    validation_steps = nb_validation_sample // batch_size
)

